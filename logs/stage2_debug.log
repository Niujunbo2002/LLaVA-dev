OpenCLIP not installed
[2025-06-02 13:12:17,769] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-02 13:12:43,006] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-02 13:12:43,006] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Rank 0:  Overwriting config with {'use_pos_skipping': False, 'pos_skipping_range': 4096, 'mm_spatial_pool_mode': 'bilinear'}
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
[92mLoading QwenViT ...[0m
[92mQwenViT loaded successfully![0m
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[92mMIN_PIXELS: 4 * 28 * 28 
MAX_PIXELS: 2048 * 28 * 28[0m
[92mMIN_PIXELS: 4 * 28 * 28 
MAX_PIXELS: 7290 * 28 * 28[0m
Rank 0:  Prompt version: qwen_1_5
Rank 0:  Using mm_tunable_parts: mm_vision_tower,mm_mlp_adapter,mm_language_model
Rank 0:  Total parameters: ~1161.48 MB)
Rank 0:  Trainable parameters: ~1161.48 MB)
‰ª£ÁêÜË¢´ÂÖ≥Èó≠
s3 Client ÂàõÂª∫ÊàêÂäü
Rank 0:  Loading /mnt/hwfile/opendatalab/bigdata_mineru/niujunbo/dataset/Stage2/Mineru_exam_zh-140k.json with first:10% sampling strategy
Rank 0:  Loaded 14367 samples from /mnt/hwfile/opendatalab/bigdata_mineru/niujunbo/dataset/Stage2/Mineru_exam_zh-140k.json
Rank 0:  Loaded 14367 samples from /mnt/petrelfs/niujunbo/zhengyuanhong/NativeResLLaVA_mineru/scripts/train/Qwen_stage2_debug.yaml
Rank 0:  Formatting inputs...Skip in lazy mode
Rank 0:  Setting NCCL timeout to INF to avoid running errors.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using /mnt/petrelfs/niujunbo/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/petrelfs/niujunbo/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 1.4578609466552734 seconds
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: zyh2022200727 (zyh2022200727-china) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /mnt/petrelfs/niujunbo/zhengyuanhong/NativeResLLaVA_mineru/wandb/run-20250602_131645-9zvxzs9v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llavanext-_mnt_hwfile_mllm_niujunbo_model-image_Qwen_Qwen2-VL-2B-Instruct-_mnt_hwfile_mllm_niujunbo_model-image_Qwen_Qwen2-0.5B-Instruct-stage1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zyh2022200727-china/huggingface
wandb: üöÄ View run at https://wandb.ai/zyh2022200727-china/huggingface/runs/9zvxzs9v
  0%|          | 0/7184 [00:00<?, ?it/s]