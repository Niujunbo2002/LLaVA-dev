<|box_start|>023 014 978 096<|box_end|><|ref_start|>table_caption<|ref_end|><|md_start|>Table 1: Main results on 9 popular benchmarks. #Data: accumulated multimodal training data volume, MaxRes.: maximum resolution supported, TFLOPs: computation cost of processing maximum resolution images, AR.: aspect ratio supported, \(\Delta\) : improvements over LLaVA-1.5 backbone.<|md_end|>
<|box_start|>026 111 969 411<|box_end|><|ref_start|>table<|ref_end|><|md_start|><fcel>Model<fcel>#Data<fcel>MaxRes.<fcel>AR.<fcel>TFLOPs<fcel>VQAv2<fcel>GQA<fcel>VQAT<fcel>POPE<fcel>SQA<fcel>VizWiz<fcel>MME<fcel>MMB<fcel>MMBCN<nl>
<fcel>BLIP-2 [21]<fcel>129M<fcel>224×224<fcel>Fix<fcel>1.0<fcel>41.0<fcel>42.5<fcel>85.3<fcel>61.0<fcel>19.6<fcel>1293.8<fcel>-<fcel>-<nl>
<fcel>InstructBLIP [11]<fcel>130M<fcel>224×224<fcel>Fix<fcel>1.0<fcel>-<fcel>49.5<fcel>50.7<fcel>78.9<fcel>63.1<fcel>33.4<fcel>1212.8<fcel>-<fcel>-<nl>
<fcel>Shikra [8]<fcel>6M<fcel>224×224<fcel>Fix<fcel>8.0<fcel>77.4<fcel>-<fcel>-<fcel>-<fcel>-<fcel>-<fcel>58.8<fcel>-<nl>
<fcel>Qwen-VL [5]<fcel>1.4B<fcel>448×448<fcel>Fix<fcel>9.2<fcel>78.8<fcel>59.3<fcel>63.8<fcel>-<fcel>67.1<fcel>35.2<fcel>-<fcel>38.2<fcel>7.4<nl>
<fcel>SPHINX [24]<fcel>1.0B<fcel>448×448<fcel>Fix<fcel>39.7<fcel>78.1<fcel>62.6<fcel>51.6<fcel>80.7<fcel>69.3<fcel>39.9<fcel>1476.1<fcel>66.9<fcel>56.2<nl>
<fcel>SPHINX-2k [24]<fcel>1.0B<fcel>762×762<fcel>Fix<fcel>69.4<fcel>80.7<fcel>63.1<fcel>61.2<fcel>87.2<fcel>70.6<fcel>44.9<fcel>1470.7<fcel>65.9<fcel>57.9<nl>
<fcel>MiniGPT-v2 [7]<fcel>326M<fcel>448×448<fcel>Fix<fcel>4.3<fcel>-<fcel>60.1<fcel>-<fcel>-<fcel>-<fcel>53.6<fcel>-<fcel>-<fcel>-<nl>
<fcel>Fuyu-8B [6]<fcel>-<fcel>1024×1024<fcel>Any<fcel>21.3<fcel>74.2<fcel>-<fcel>-<fcel>74.1<fcel>-<fcel>-<fcel>728.6<fcel>10.7<fcel>-<nl>
<fcel>OtterHD-8B [20]<fcel>-<fcel>1024×1024<fcel>Any<fcel>21.3<fcel>-<fcel>-<fcel>-<fcel>86.0<fcel>-<fcel>-<fcel>1223.4<fcel>58.3<fcel>-<nl>
<fcel>mPLUG-Owl2 [43]<fcel>401M<fcel>448×448<fcel>Fix<fcel>1.7<fcel>79.4<fcel>56.1<fcel>58.2<fcel>86.2<fcel>68.7<fcel>54.5<fcel>1450.2<fcel>64.5<fcel>-<nl>
<fcel>UReader [42]<fcel>86M<fcel>896×1120<fcel>Enum<fcel>26.0<fcel>-<fcel>-<fcel>57.6<fcel>-<fcel>-<fcel>-<fcel>-<fcel>-<fcel>-<nl>
<fcel>Monkey [23]<fcel>1.0B<fcel>896×1344<fcel>Enum<fcel>65.3<fcel>80.3<fcel>60.7<fcel>-<fcel>67.6<fcel>69.4<fcel>61.2<fcel>-<fcel>-<fcel>-<nl>
<fcel>LLaVA-1.5 [27]<fcel>1.2M<fcel>336×336<fcel>Fix<fcel>15.5<fcel>80.0<fcel>63.3<fcel>61.3<fcel>85.9<fcel>71.6<fcel>53.6<fcel>1531.3<fcel>67.7<fcel>63.6<nl>
<fcel>LLaVA-UHD (ours)<fcel>1.2M<fcel>672×1008<fcel>Any<fcel>14.6<fcel>81.7<fcel>65.2<fcel>67.7<fcel>89.1<fcel>72.0<fcel>56.1<fcel>1535.0<fcel>68.0<fcel>64.8<nl>
<fcel>Δ<fcel>-<fcel>×6 times<fcel>-<fcel>-0.9<fcel>+1.7<fcel>+1.9<fcel>+6.4<fcel>+3.2<fcel>+0.4<fcel>+2.5<fcel>+3.7<fcel>+0.3<fcel>+1.2<nl>
<|md_end|>
<|box_start|>024 468 208 495<|box_end|><|ref_start|>title<|ref_end|><|md_start|># 4.3 Main Results<|md_end|>
<|box_start|>023 519 973 985<|box_end|><|ref_start|>text<|ref_end|><|md_start|>We report the main experimental results in Table 1, from which we have the following observations: (1) LLaVA- UHD outperforms strong baselines on popular benchmarks. This includes strong general baselines trained on 2- 3 orders of magnitude more data such as Qwen- VL and InstructBLIP, and also high- resolution LMMs that require significantly more computation such as Fuyu- 8B, OtterHD- 8B, Monkey and SPHINX- 2k. The results show that LLaVA- UHD can properly deal with native- resolution images for strong performance, as well as good data and computation efficiency. (2) LLaVA- UHD achieves significant improvements over the LLaVA- 1.5 backbone. Notably, by simply perceiving images in native high- resolution, LLaVA- UHD achieves 6.4 accuracy improvement on TextVQA and 3.2 accuracy improvement on POPE. The reason is that the blurred content in low- resolution images can prevent LMMs from accurately identifying the challenging fine- grained objects and optical characters. The results demonstrate the fundamental role of perceiving native high- resolution images in various multimodal tasks, and the effectiveness of LLaVA- UHD in addressing the problem. (3) In terms of resolution and efficiency, compared with LLaVA- 1.5 associated fixed \(336\times 336\) resolution, LLaVA- UHD supports \(672\times 1088\) resolution images in any aspect ratio using only \(94\%\) inference computation. The results indicate promising scalability of LLaVA- UHD to potentially larger resolutions in future.<|md_end|>